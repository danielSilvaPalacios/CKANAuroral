import ckan.plugins as plugins
import ckan.plugins.toolkit as toolkit
import os
import ckanapi
from ckanapi import RemoteCKAN
from apscheduler.schedulers.background import BackgroundScheduler
import requests
import json
import re
import random
import string
from datetime import datetime
import logging
import threading
import yaml
import schedule
import time

# AURORAL INTEGRATION
# Script to obtain the metadata of the datasets and organizations of the community and register them in CKAN 



lock = threading.Lock()


dataset_descriptions = []
organization_descriptions = []


#Variables to access the ckan api
ckan_url=os.getenv('CKAN_SITE_URL')
ckan_key=os.getenv('CKAN_API_KEY')



#_______________SEPARATE ITEMS OIDS______________

def get_unique_ids(array): 
    
    """
    Obtain the OIDs (object IDs) of the objects contained in the nodes to separate the metadata.

    Parameters:
    - array: The input array containing the objects.

    Returns:
    - A tuple containing a list of unique IDs and a list of unique ID strings.
    """
    
    
    unique_ids = []
    unique_id_strings = []
    for item in array:
        # Obtain the ID after "graph:"
        _, id_value = item.split('https://oeg.fi.upm.es/wothive/')

        # Verify if the ID is in the list of unique_ids
        if id_value not in unique_ids:
            unique_ids.append(id_value)
            unique_id_strings.append("https://oeg.fi.upm.es/wothive/" + id_value)
    print(f"____2____ unique_id_string{unique_ids}")
    
    return unique_ids, unique_id_strings





#_______________CHECK SYNTAX______________

def process_string(input_string):
    
    """
    Process a string to avoid special characters and ensure it conforms to the required format to
    avoid the error: ust be purely lowercase alphanumeric (ascii) characters and these symbols: -_
    when registering datasets and organizations in CKAN.

    Parameters:
    - input_string: The input string to be processed.

    Returns:
    - The processed string with special characters removed.
    """
    
    
    # Remove whitespace
    processed_string = input_string.replace(" ", "")

    # Convert to lowercase
    processed_string = processed_string.lower()
    
    # Apply regular expression to remove unwanted characters
    processed_string = re.sub(r"[^a-z0-9\-_'']", "", processed_string)
    
    print(f"____3____ unique_id_string{process_string}")
    
    return processed_string





#_______________OBTAIN TD FROM TRIPLES______________

#filter the triples by the oid of the object and obtain the metadata modeled in the tiples
def obtain_metadata(response):
    dataset_descriptions = []
    organization_descriptions = []
    # Step 1: Obtain a list of all "sub" values
    subs = [binding['sub']['value'] for binding in response['message']['results']['bindings'] if binding['sub']['value'].startswith('https://oeg.fi.upm.es/wothive/')]
    oids, oids_sub=get_unique_ids(subs) 
    #print(oids, oids_sub)
    #print(oids_sub)
    print("Number of Datasets: "+str(len(oids_sub)))
    
    #Create a list of datasets
    thing_descriptions = []
    
    # Step 2-4: Iterate over the "sub" values and create the thing descriptions
    for sub in oids_sub:  # Using set to obtain unique values
        # Step 2: Filter the triples for the current object
        # print("OID Dataset: "+str(sub))
        filtered_results = [binding for binding in response['message']['results']['bindings'] if sub.__contains__(binding['sub']['value'])]
        
        # Define the dictionary for the dataset information
        dataset_data = {
            'name': '',
            'title': '',
            'groups': [
                {   
                    'name': ''
                }
            ],
            'owner_org': '',
            'description': '',
            'resources': [
                {
                    'name': '',
                    'url': '',
                }
            ]
        }
        
        # Iterate over the triples and obtain the metadata values
        organization_data = {
            'name': '',
            'title': '',
            'description': ''
        }
        for triple in filtered_results:
            subject = triple['sub']['value']
            predicate = triple['p']['value']
            object_type = triple['o']['type']
            object_value = triple['o']['value']

            # Update dataset_data based on the triple and the information from the WoT TD
            if predicate == 'https://www.w3.org/2019/wot/td#title': #NAME
                dataset_data['name'] = process_string(object_value)
                organization_data['name'] = process_string(object_value)
            elif predicate == 'https://www.w3.org/2019/wot/td#serviceName': # RESOURCE - TITLE
                dataset_data['title'] = object_value
                organization_data['title'] = object_value
                dataset_data['resources'][0]['name'] = object_value
            elif predicate == 'https://www.w3.org/2019/wot/td#hasURL': #RESOURCE - URL
                dataset_data['resources'][0]['url'] = object_value
            elif predicate == 'https://www.w3.org/2019/wot/td#provider'  or predicate == 'https://www.w3.org/2019/wot/td#owner_org' or predicate == 'https://auroral.iot.linkeddata.es/def/core#provider': #OWNER ORG
                dataset_data['owner_org'] = process_string(object_value)
                organization_data['description'] = str(object_value) + " description"

            if predicate == 'https://www.w3.org/2019/wot/td#serviceDescription': #DESCRIPTION
                dataset_data['notes'] = object_value
            elif predicate == 'https://www.w3.org/2019/wot/td#hasDomain':
                dataset_data['domain'] = object_value
            elif predicate == 'https://www.w3.org/2019/wot/td#hasDomain':
                dataset_data['groups'][0]['name'] = process_string(object_value)

        # print(json.dumps(dataset_data, indent=4))
        #append datasets to a single dictionary   
        organization_descriptions.append(organization_data) 
        dataset_descriptions.append(dataset_data) 
        
    # Step 5: Return the list of datasets
    return dataset_descriptions, organization_descriptions






#_______________CREATE DATASETS ON CKAN______________

def create_datasets_CKAN(organization_data, dataset_data):
    print("Entra a create_datasets_CKAN")
    print(f"organization_data: {organization_data}")
    print(f"datasets_data: {dataset_data}")
    
    #Connect with ckan
    ckan = ckanapi.RemoteCKAN(ckan_url, apikey= ckan_key)
    
    print(f"CKAN_CREATE_DATASETS: dataset_data {dataset_data}")
    # Manage organizations
    try:
        # Check if the organization exists
        if len(dataset_data) > 0:
            print(f"Dataset data: {dataset_data}")
            val = dataset_data['owner_org']
            ckan.action.organization_show(id=val)
            print('____8____Organization already exists!')
            print()
    
    except ckanapi.NotFound:
        # If the organization doesn't exist, create it
        ckan.action.organization_create(**organization_data)
        print('____9____Organization created successfully!')
    # except ValidationError as e:
    #     print('____10____Failed to create organization:', e)

    # Create a new dataset on CKAN
    try:
        # If it does note exists, create it 
        ckan.action.package_create(**dataset_data)
        print('____11____Dataset created successfully')
    except ckanapi.errors.ValidationError:
        # If it trows an error, it means that the dataset already exists. Do nothing.
        print('____12____Dataset already exists')
    except Exception as e:
        print('____13____Failed to create dataset:', e)
        
        
    return dataset_data




    
#______________GET DATASETS FROM CKAN______________

def get_ckan_datasets():
    
    """
    #Retrieves datasets from a CKAN database using the CKAN API.    
    #Returns a list of dataset dictionaries retrieved from the CKAN database.
    
    Returns:
    - The list of datasets names retrieved from the CKAN database.
    """
    # Create a CKAN API client
    ckan = ckanapi.RemoteCKAN(ckan_url, apikey= ckan_key)
    
    try:
        # Retrieve the list of datasets
        datasets = ckan.action.package_list()
        # print(datasets)
        print(f"RECUPERA LA LISTA DE DATASETS CKAN ___{datasets}")
        return datasets

    except ckanapi.NotFound:
        print("____14____CKAN instance not found.")
    except ckanapi.NotAuthorized:
        print("____15____Not authorized to access CKAN API.")
    except ckanapi.CKANAPIError:
        print("____16____An error occurred while retrieving datasets.")

    return []




#______________DELETE DATASETS FROM CKAN______________

def delete_dataset_CKAN(dataset_name):
    
    """
    Deletes a dataset from CKAN.

    Parameters:
    - dataset_name: The name or ID of the dataset to be deleted.

    Returns:
    - None
    """


    #Create a CKAN API client
    ckan = ckanapi.RemoteCKAN(ckan_url, apikey= ckan_key)

    try:
        # Delete the dataset
        ckan.action.package_delete(id=dataset_name)
        print(f"____17____Dataset '{dataset_name}' deleted from CKAN.")
    except ckanapi.CKANAPIError as e:
        print(f"____18____An error occurred while deleting dataset '{dataset_name}': {e}")
        
        


# API base URL
net_ckan_node = "http://aur-1695287971-proxy-1:8080"

base_url = f"{net_ckan_node}/api/discovery"
base_url_post = f"{net_ckan_node}/api/discovery/remote/semantic"  # = f"{base_url}/remote/semantic"

# Common headers
headers_get = {'accept': 'application/json'}
headers_post = {'accept': 'application/json', 'Content-Type': 'text/plain'}
headers = {'accept': 'application/json'}

# Obtain Community ID
urlcommId = f"{net_ckan_node}/api/collaboration/communities"
payload = {}
headers = {
    'Accept': 'application/json',
    'Content-Type': 'text/plain'
}
response = requests.request("GET", urlcommId, headers=headers_get)
commId = response.json()
commId = commId['message'][0]['commId']

def data_org():
    print("base_url: "+str(base_url))
    print("commId: "+str(commId))
    # Request to obtain the AGIDs
    response = requests.get(f"{base_url}/nodes/community/{commId}", headers=headers_get)

    datasets = []
    organization = []
    
    list_datasets =[]
    list_organizations = []
    
    if response.status_code == 200:
        print("____0____Request to obtain the AGIDs: OK")
        data = response.json()
        messages = data.get('message', [])
        
        # Iterar a través de cada mensaje para obtener el agid
        for message in messages:
            agid = message.get('agid')
            print("______________________________________________")
            print("-----AGID------ "+(agid))        
            # Realizar una nueva solicitud GET para cada agid
            response_oid = requests.get(f"{base_url}/remote/semantic/predefined/getOids?agids={agid}", headers=headers)
            
            if response_oid.status_code == 200:
                print("RESPUESTA 200: OK")
                oids_data = response_oid.json()
                bindings = oids_data.get('message', {}).get('results', {}).get('bindings', [])
                print("bindings: "+str(bindings))
                
                # Contar y mostrar el número de oids
                num_oids = len(bindings)
                print(f"Number of OIDs: {num_oids}")
                
                # Extract and print the names of each oid
                for binding in bindings:
                    print("Entra a bucle de OIDS")
                    oid_value = binding.get('oid', {}).get('value', '')
                    oid_name = binding.get('name', {}).get('value', '')
                    print(f"OID: {oid_value}, Name: {oid_name}")
                    

                # Realizar una solicitud POST para cada agid
                query = '''
                PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
                PREFIX wot: <https://www.w3.org/2019/wot/td#> 
                SELECT distinct ?p ?o ?sub WHERE { 
                ?sub rdf:type wot:Service . 
                ?sub ?p ?o .}
                '''
                response_post = requests.post(f"{base_url_post}?agids={agid}", headers=headers_post, data=query)

                # Verificar si la solicitud POST fue exitosa
                if response_post.status_code == 200:
                    post_data = response_post.json()
                    # Obtain the metadata of all the datasets 
                    datasets, organization = obtain_metadata(post_data)
                    
                    list_datasets.append(datasets)
                    list_organizations.append(organization)

                    for data in datasets:
                        name = data["name"]
                        title = data["title"]
                        domain = data["domain"]
                        description = data["description"]
                        owner_org = data["owner_org"]
                        resources_name = data["resources"][0]["name"]
                        resources_url = data["resources"][0]["url"]
                                            
                        print(json.dumps(data, indent=4))
                    
                    num_items = len(datasets)
                
                    
                    #print(f"POST response for agid {agid}: {post_data}")
                
            else:
                print(f"Error in POST semantic request for agid {agid}: {response_post.status_code}") #when consulting the agid of a local node from a remote query, it returns an error

        
        print(f"Total number of datasets: " + str(num_items))
         
    else:
        print(f"Error querry AGID Communities: {response.status_code}")
        
    print(f"datasets: {list_datasets}")
    print(f"organization: {list_organizations}")
    return list_datasets, list_organizations  



        
#______________UPDATE DATASETS ON CKAN BASED ON THE REGISTERED ITEMS IN THE NODES______________

def update_datasets():

    """
    Updates the datasets in CKAN based on the obtained results 
    from a SPARQL query to the AURORAL community.

    Parameters:
    - None

    Returns:
    - None
    """
    print("Entra a update_datasets")
    # # Send SPARQL query request and process the information to create a list of 
    # # datasets and its metadata on the nodes participating into the AURORAL community
    # response = requests.post(f"{base_url_post}?agids={agid}", headers=headers_post, data=query)
    # # response = requests.post(node_url, headers=node_headers, data=query)

    # # Parse response JSON
    # data = response.json()
    # print("____18____Data: "+str(data)) 

    # # Obtain the metadata of all the datasets using the funcion obtain_metadata
    # datasets, organizations=obtain_metadata(data)
    # # for data in datasets:
    # print("Pasa por data")
    # print(data)
    # print("Ha pasado por data")
    # # Get the existing datasets in CKAN
    existing_datasets = get_ckan_datasets()

    # Create a list of existing dataset names
    print("____19____Existing dataset names: " + str(existing_datasets))

    datasets, organizations = data_org()
    print("Algooooooooooooooooo")            

    # Check if the dataset exists in CKAN and delete if not found in the obtained results
    for dataset_name in existing_datasets:
        if dataset_name not in [data['name'] for data in datasets]:
            print(f"____20____Dataset '{dataset_name}' not found in the obtained results. Deleting from CKAN...")
            delete_dataset_CKAN(dataset_name)
            
    print("____21____Dataset deletion completed.")
    
    print("datasets: "+str(datasets))
    print("organizations: "+str(organizations))
    # #For every dataset obtained from the query to the community, call to the funtion create_datasets_CKAN to create the datasets on CKAN
    for data, orga in zip(datasets, organizations):
        print("Entra a bucle de datasets")
        """
        print("_________Obtained Resoruces (Datasets)_______")
        print(json.dumps(data, indent=4))
        print(json.dumps(organizations, indent=4))

        #create_datasets_CKAN to create the datasets on CKAN
        print("_________Dataset creation________________")
        """
        create_datasets_CKAN(orga, data)







# #Url and headers for the sparql query to the nodes of the community
# # node_url = 'http://aur-node_5dd32b3e-proxy-1:8080/api/discovery/local/semantic' #change to put the url of the remote query!!
# node_url = f"{net_ckan_node}/api/discovery/remote/semantic/community/{commId}"

# node_headers = {
#     'Accept': 'application/json',
#     'Content-Type': 'text/plain'
# }


def job():
    with lock:
        print("Entra a job")
        update_datasets()

scheduler_started = False

def run_scheduler():
    global scheduler_started
    if scheduler_started:
        return
    scheduler_started = True
    schedule.every(1).minutes.do(job) #change to xx minutes

    while True:
        schedule.run_pending()
        time.sleep(1)


def read_yaml():
    # with open('/srv/app/src_extensions/ckanext-testapi/ckanext/testapi/control.yaml', 'r') as file:
    with open('/srv/app/src_extensions/ckanext-auroral_integration/ckanext/auroral_integration/control.yaml', 'r') as file:
        data = yaml.safe_load(file)
    return data
    
def save_yaml(data):
    # with open('/srv/app/src_extensions/ckanext-testapi/ckanext/testapi/control.yaml', 'w') as file:
    with open('/srv/app/src_extensions/ckanext-auroral_integration/ckanext/auroral_integration/control.yaml', 'w') as file:
        yaml.dump(data, file)





class AuroralIntegrationPlugin(plugins.SingletonPlugin):
    plugins.implements(plugins.IConfigurer)

    # IConfigurer
    def update_config(self, config_):
        print("update config")
        toolkit.add_template_directory(config_, 'templates')
        toolkit.add_public_directory(config_, 'public')
        toolkit.add_resource('fanstatic','auroral_integration')  
        
        control = read_yaml()
        if control["val"] == 0:       
            threading.Thread(target=run_scheduler).start()
            print("_____________________Scheduler started")
        else:
            print("_____________________Control bucle")
          

        # #Scheduler to update the datasets on CKAN
        # self.scheduler = BackgroundScheduler()   
   
        # self.scheduler.add_job(
        #     update_datasets,  # The function to run
        #     'interval',  # Run the job at a specified interval
        #     seconds=60 # Run the job every X time
        # )
        
        # self.scheduler.start()
 
 